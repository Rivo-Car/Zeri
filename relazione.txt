RELAZIONE GRUPPO ALPHA 


IL PROBLEMA DEL CALCOLO DEGLI ZERI DI UNA FUNZIONE 



1) METODO DI BISEZIONE 


	*** IPOTESI *** :

	Data l'equazione f(x)=0 definita e continua in un intervallo [a,b], tale che f(a)*f(b) < 0 , è allora possibile calcolarne un'approssimazione in [a,b] 		(secondo il teorema degli zeri).


	*** PROCEDIMENTO *** : 

	Si procede dividendo l'intervallo in due parti uguali e calcolando il valore della funzione nel punto medio di ascissa m = (a+b)/2.
	Se risulta f(m) = f((a+b)/2) = 0 allora m è la radice cercata; altrimenti tra i due intervalli [a, m) e (m, b] si sceglie quello ai cui estremi la funzione 		assume 	valori di segno opposto. Si ripete per questo intervallo il procedimento di dimezzamento.
	Così continuando si ottiene una successione di intervalli [a1, b1], [a2, b2], ..., [an, bn] incapsulati, cioè ognuno incluso nel precedente. Questi intervalli 		hanno come ampiezze bn - an = (b-a) / 2^n per n = 1,2,...

	I valori ai sono valori approssimati per difetto della radice, i valori di bi sono invece i valori della radice approssimati per eccesso. 
	Gli an formano una successione crescente limitata ed i bn formano una successione decrescente limitata.
	Le due successioni ammettono lo stesso limite che è la radice dell'equazione esaminata.

	Come approssimazione della radice alpha si considera il punto medio degli intervalli, cioè mn = (an + bn) / 2 , per n = 1,2,...


	*** CRITERIO DI ARRESTO E ERRORE *** :

	L'algoritmo viene arrestato quando f(mn) è abbastanza vicino a 0 e/o quando l'ampiezza dell'intervallo [an, bn] è inferiore ad una certa tolleranza toll_err.
	Dunque come stima di alpha  alla fine avremo un certo mn. 

	Si dimostra facilmente che per l'errore commesso e_n vale la seguente relazione:

		 |e_n| = |c_n - alpha| <= (b-a) / (2 ^ (n+1)).


	*** CONVERGENZA DEL METODO *** :


	** VANTAGGI : 
	
	1- Vale la seuente relazione:

		lim (n->+inf) |e_n| = 0

	quindi la convergenza del metodo è globale (converge sempre); è quindi un metodo molto robusto.
	
	2- Ha il notevole pregio di essere stabile in ogni occasione e quindi di garantire sempre la buona riuscita dell'operazione.


	2- Dal p.d.v. del costo computazionale, poichè ad ogni iterazione si esegue una sola valutazione di funzione, il costo computazionale risulta essere basso. 


	


	** SVANTAGGI :

	1- L'efficienza del metodo di bisezione è scarsa e presenta lo svantaggio di richiedere ipotesi particolarmente restrittive. 

	2- Se richiediamo |e_n| <= toll_err otteniamo la seguente condizione per n:

		n >= log_2 ((b-a) / toll_err) - 1.
	
	Essendo log_2 10 =ca 3.32  servono in media più di tre bisezioni per migliorare di una cifra significativa l'accuratezza della radice, quindi la convergenza è 		lenta. 

	3- Inoltre la riduzione dell'errore a ogni passaggio non è monotona, cioè non è detto che: |e_n+1| < |e_n per ogni n = 1,2,...|.

	4- Non si può definire quindi un vero e proprio ordine di convergenza per questo metodo.



	*** FONTI ***:

	https://it.wikipedia.org/wiki/Metodo_della_bisezione

	Dispensa_ricercaZeri.pdf
	


2) METODO DI NEWTON 


	- VERSIONE UNIDIMENSIONALE -


	*** IPOTESI *** :
	
	La condizione necessaria affinché il metodo sia applicabile è che esista un intervallo [a,b] in cui x0 nell'intervallo è tale che f'(x0) != 0 e f''(x0) != 0 		(teorema degli zeri).


	*** PROCEDIMENTO *** : 

	Il metodo delle tangenti, chiamato anche metodo di Newton o metodo di Newton-Raphson, è uno dei metodi per il calcolo approssimato di una soluzione di 		un'equazione della forma f(x)=0. Esso si applica dopo avere determinato un intervallo [a,b] che contiene una sola radice.
	Il metodo consiste nel sostituire alla curva y=f(x) la tangente alla curva stessa, partendo da un qualsiasi punto: considerando l’equazione del fascio di 	rette: y-f(x0)=m(x-x0), posto y=0 e m=f'(x0), x1 sarà: x1=x0-(f(x0)/f'(x0))

	Generalizzando all’iterazione k: x_k-1 = x_k - (f(x_k))/f'(x_k))), che è la relazione di ricorrenza del metodo (che permette di determinare successive 	approssimazioni della radice dell'equazione y=f(x)=0).

	
	*** CRITERIO DI ARRESTO E ERRORE *** :


	*** CONVERGENZA DEL METODO *** :

	** VANTAGGI :

	1-  Con le ipotesi poste, si dimostra che la successione delle x_n converge alla radice piuttosto rapidamente.
	Più in dettaglio, si dimostra che se f appartiene a C^2(I) dove I è un opportuno intorno dello zero alpha con f'(alpha) != 0 e se x0 appartiene a I, allora:

			lim (n->+inf) (alpha - x_n+1) / (alpha - x_n)^2 = - f''(alpha) / 2*f'(alpha).

	cioè la convergenza è quadratica (il numero di cifre significative approssimativamente raddoppia ad ogni iterazione.
	
	Si può subito notare la velocità di convergenza del metodo rispetto al metodo precedente: col metodo di bisezione cresce linearmente, benché locale (cioè non 		vale per ogni I). 

	2- Il metodo esegue un numero di iterazioni significativamente inferiore. 





	** SVANTAGGI :
	
	1- Il metodo è applicabile solo a funzioni derivabili con derivata prima diversa da zero nei punti della successione. 

	2- La convergenza non è garantita, in particolare quando f'(x) varia notevolmente in prossimità dello zero. 
	Inoltre, il metodo assume che f'(x) sia disponibile direttamente per un dato x. Nei casi in cui questo non si verifichi e risultasse necessario calcolare la 	derivata attraverso una differenza finita, è consigliabile usare il metodo delle secanti.

	3- Il numero di iterazioni infatti dipende principalmente dalla posizione del punto iniziale rispetto allo zero. 
	In base alla funzione analizzata e al valore di x0 assunto come prima iterazione il metodo può convergere o meno. Il metodo risulta meno solido rispetto al 	metodo di bisezione.

	4- Come costo computazionale, ad ogni iterazione esegue due valutazioni di funzione. È quindi più pesante rispetto al precedente metodo. 

	




	- VERSIONE MULTIDIMENSIONALE - 

		Se invece la radice è multipla, cioè f'(alpha)=0 allora la convergenza è lineare (più lenta).

	Nella pratica, fissata la tolleranza di approssimazione consentita toll_err , il procedimento iterativo si fa terminare quando:

		|x_n+1 - x_n| < toll_err * |x_n+1|.





	
3) METODO DELLE CORDE 

		
	//da dispense prof Marchetti:

	niente di utile maledetto:(



	//da wikipedia pag.principale metodo delle corde 
	
	
	*** IPOTESI *** :
	
	Il metodo delle corde (o metodo delle secanti con estremo fisso) è uno dei metodi più semplici per il calcolo approssimato di una soluzione di un'equazione 			della forma f(x)=0.
 	Esso si applica dopo avere determinato un intervallo [a,b] che contiene una sola radice.
	
	*** PROCEDIMENTO ***:

	Il metodo consiste nel costruire una successione di punti con il seguente criterio:
	
	assegnato un punto iniziale x_0, per ogni n >= 0, il punto x_n+1 sia lo zero della retta passante per il punto (x_n, f(x_n) e di coefficiente angolare m = (f(a)-f(x_n) / (a-x_n)
	ovvero quello della retta passante per i punti (x_n,f(x_n)) e (a,f(a)).

	Iterando il procedimento del calcolo dell'intersezione delle varie rette con l'asse delle ascisse, si ottiene la relazione di ricorrenza:

	x_n+1} = a - f(a)*(x_n - a) / (f(x_n) - f(a)).
	
	
	*** CRITERIO DI ARRESTO ED ERRORE *** :
	
	Solito discorso tolleranza e iterazioni sicuramente, vai a controllare. --> |x_k| < precisione/tolleranza scelta
	
	
	*** ANALISI *** :
	
	
	** VANTAGGI: 
	
	
	
	** SVANTAGGI :

	Il metodo delle corde converge linearmente se, detta "alpha"  la soluzione corretta, vale: 

		0 < f'(alpha)/ m < 2


	In altri termini, m e f'(alpha) devono avere lo stesso segno e l'intervallo [a,b] deve soddisfare la condizione:

			b-a < 2 * ((f(b) - f(a)) / f'(alpha)).

	Negli altri casi il metodo potrebbe non convergere affatto.




4) METODO DELLE SECANTI

	
	//da dispense prof Marchetti:

	niente di utile maledetto PT 2:(



	//da wikipedia pag.principale metodo delle secanti


	Il metodo delle secanti (o metodo delle secanti con estremi variabili) è uno dei metodi più semplici per il calcolo approssimato di una soluzione di 	un'equazione della forma f(x)=0. Esso si applica dopo avere determinato un intervallo [a,b], che contiene una sola radice.

	Il metodo consiste nel costruire una successione di punti con il seguente criterio: assegnati due punti iniziali x0 e x1, per ogni n >= 1 , il punto 	x_n+1 sia lo zero della retta passante per i punti (x_n-1,f(x_n-1)), (x_n, f(x_n)). Si ottiene:

	x_n+1 = x_n - ((x_n - x_n-1) / (f(x_n) - f(x_n-1))) * f(x_n).

	Rispetto al metodo delle corde, quello delle secanti richiede un punto iniziale in più e ad ogni passo il calcolo del rapporto che compare nella formula. 	
	Inoltre la convergenza è locale, cioè dipende dalla scelta dei punti iniziali x0 e x1; il guadagno è però una maggiore velocità di convergenza, che risulta 	superlineare.

	Si dimostra infatti che, detta alpha la soluzione corretta, se f appartiene a C^2 ([a,b]) e f''(alpha) != 0 e x0 e x1 sono abbastanza vicini ad alpha ,
	allora il metodo converge con ordine:

		p = (1+sqrt(5)) / 2 =ca 1,618.



*** VANTAGGI:
1) si può usare quando non si conosce la derivata di f(x) o quando f(x) è nota per punti;
2) ad ogni passo richiede una sola valutazione di funzione, quindi il costo computazionale per iterata è veramente molto basso.

*** SVANTAGGI:
1) servono 2 approssimazioni iniziali x0 e x1;
2) la scelta di x0 e x1 deve essere "accurata".


                 - - - - - - - - - - - - - - - - - - - - - - - - -- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -- -


*** EFFICIENZA COMPUTAZIONALE DEGLI ALGORITMI ***


Per valutare l'efficienza di un metodo iterativo bisogna tener conto sia dell'*ordine di convergenza* che del *costo computazionale*, cioè dalla quantità di calcoli richiesta ad 
ogni passo.

	efficienza_computazionale: E = p^(1/r)        dove p: ordine di convergenza del metodo
						           r: numero di valutazioni funzionali (calcolo di funzioni o derivate) richieste ad ogni passo
							   
- metodo di bisezione: E = 1 (ad ogni passo si richiede una sola valutazione funzionale f(xk)) e quindi r = 1);
- metodo di Newton: E = 2^(1/2) (ad ogni passo si richiedono due valutazioni funzionali f(xk) e f'(xk) e quindi r = 2);
- metodo delle corde: E = ??
- metodo delle secanti: E = { 1 < p < 2 se f'(x) != 0     --> convergenza superlineare 
			    { (1 + sqrt(5)) / 2 se f''(x) != 0 

(ad ogni passo si richiede una sola valutazione funzionale f(xk) e quindi r = 1);



*** svantaggi metodo corde:
1) la convergenza del metodo è molto legata al tipo di funzione e alla scelta del punto di partenza : se quest'ultimo non è abbastanza vicino alla radice o la funzione non è 
abbastanza regolare, il metodo può convergere molto lentamente.

*** vantaggi metodo corde:
1) il metodo presenta un basso costo computazionale per iterazione (si valuta solo una volta f'(x) e una sola volta per iterazione f(x)), ma si può dimostrare che l'ordine di 
convergenza è solo lineare.



	

 

	
	

	
